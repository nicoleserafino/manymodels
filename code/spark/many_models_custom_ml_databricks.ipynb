{"cells":[{"cell_type":"markdown","metadata":{},"source":["#Technique in Spark to train multiple models and perform scalable inferencing"]},{"cell_type":"markdown","metadata":{},"source":["## 1.Preperation"]},{"cell_type":"markdown","metadata":{},"source":["### Environment preperation\n","1. Prepare a Databricks instance with Ls8s_v2\n","2. Prepare a Azure ML workspace \n","3. Prepare a service principal with secret key registered in keyvault. The service principal should have contributor access to your Azure ML workspace"]},{"cell_type":"markdown","metadata":{},"source":["### Download data from Microsoft Open Dataset"]},{"cell_type":"markdown","metadata":{},"source":["https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["data =spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://ojsales-simulatedcontainer@azureopendatastorage.blob.core.windows.net/oj_sales_data/Store10*.csv\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#Write to local delta for fast reading\n","data.write.format(\"delta\").saveAsTable(\"OJ_Sales_Data\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td>null</td><td>List(1, 16, List(456483, 456483, 456483.0, 1, 456483), List(29570, 37082, 36254.5625, 16, 580073), 0, List(minCubeSize(107374182400), List(0, 0), List(16, 580073), 0, List(16, 580073), 1, null), 1, 16, 0, false)</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["%sql optimize OJ_Sales_Data zorder by store, brand"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Store</th><th>Brand</th><th>Quantity</th><th>Advert</th><th>Price</th><th>Revenue</th></tr></thead><tbody><tr><td>1990-06-14</td><td>1094</td><td>minute.maid</td><td>17892</td><td>1</td><td>2.09</td><td>37394.28</td></tr><tr><td>1990-06-21</td><td>1094</td><td>minute.maid</td><td>14053</td><td>1</td><td>2.45</td><td>34429.850000000006</td></tr><tr><td>1990-06-28</td><td>1094</td><td>minute.maid</td><td>17341</td><td>1</td><td>2.47</td><td>42832.270000000004</td></tr><tr><td>1990-07-05</td><td>1094</td><td>minute.maid</td><td>17194</td><td>1</td><td>2.42</td><td>41609.479999999996</td></tr><tr><td>1990-07-12</td><td>1094</td><td>minute.maid</td><td>17945</td><td>1</td><td>2.39</td><td>42888.55</td></tr><tr><td>1990-07-19</td><td>1094</td><td>minute.maid</td><td>17371</td><td>1</td><td>2.3</td><td>39953.299999999996</td></tr><tr><td>1990-07-26</td><td>1094</td><td>minute.maid</td><td>9825</td><td>1</td><td>2.36</td><td>23187.0</td></tr><tr><td>1990-08-02</td><td>1094</td><td>minute.maid</td><td>10849</td><td>1</td><td>2.58</td><td>27990.420000000002</td></tr><tr><td>1990-08-09</td><td>1094</td><td>minute.maid</td><td>12084</td><td>1</td><td>2.0</td><td>24168.0</td></tr><tr><td>1990-08-16</td><td>1094</td><td>minute.maid</td><td>10484</td><td>1</td><td>2.32</td><td>24322.879999999997</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["%sql select * from OJ_Sales_Data limit 10"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(DISTINCT store, brand)</th></tr></thead><tbody><tr><td>300</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["%sql select count (distinct store, brand) from OJ_Sales_Data "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>brand</th></tr></thead><tbody><tr><td>dominicks</td></tr><tr><td>tropicana</td></tr><tr><td>minute.maid</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["%sql select distinct brand from OJ_Sales_Data "]},{"cell_type":"markdown","metadata":{},"source":["## Pre-training exersize"]},{"cell_type":"markdown","metadata":{},"source":["1. Read about Pandas Function APIs: https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/pandas-function-apis\n","2. Answer following questions:\n","  - What is the advantage of this technology vs. regular Python UDF?\n","  - What is the role of Apache Arrow in this?\n","  - What is the use of iterator and yield vs. regular list and return?"]},{"cell_type":"markdown","metadata":{},"source":["Using the OJ sales dataset above, use Pandas Function APIs, pick out for each store and brand the best selling week in the form of week_number-yyyy.\n","The result set look like this:"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store</th><th>Brand</th><th>Best_Selling_Week</th></tr></thead><tbody><tr><td>1066</td><td>dominicks</td><td>23-1992</td></tr><tr><td>1067</td><td>tropicana</td><td>24-1991</td></tr><tr><td>1068</td><td>tropicana</td><td>24-1991</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","result_sample= pd.DataFrame({\"store\": [1066, 1067, 1068],'Brand':['dominicks', 'tropicana','tropicana'],\"Best_Selling_Week\": ['23-1992', '24-1991','24-1991']})\n","display(result_sample)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>Brand</th><th>Best_Selling_Week</th><th>Qty</th></tr></thead><tbody><tr><td>1031</td><td>tropicana</td><td>48-1991</td><td>19916.0</td></tr><tr><td>1021</td><td>minute.maid</td><td>11-1991</td><td>19947.0</td></tr><tr><td>1074</td><td>tropicana</td><td>38-1991</td><td>19932.0</td></tr><tr><td>1077</td><td>minute.maid</td><td>15-1992</td><td>19934.0</td></tr><tr><td>1078</td><td>minute.maid</td><td>44-1991</td><td>19978.0</td></tr><tr><td>1019</td><td>minute.maid</td><td>41-1991</td><td>19685.0</td></tr><tr><td>1090</td><td>tropicana</td><td>44-1990</td><td>19997.0</td></tr><tr><td>1099</td><td>tropicana</td><td>30-1990</td><td>19576.0</td></tr><tr><td>1014</td><td>minute.maid</td><td>32-1991</td><td>19995.0</td></tr><tr><td>1020</td><td>minute.maid</td><td>43-1991</td><td>19996.0</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#The Pandas function\n","import pandas as pd\n","def best_selling_week(inputdf):\n","  store =inputdf['Store'][0]\n","  brand = inputdf['Brand'][0]\n","  best_week_row = inputdf.iloc[inputdf['Quantity'].argmax()]\n","  best_week =str(best_week_row['WeekStarting'].isocalendar()[1]) +\"-\"+ str(best_week_row['WeekStarting'].isocalendar()[0])\n","  qty = best_week_row['Quantity']\n","\n","  return pd.DataFrame({\"Store\":[store], \"Brand\":[brand], \"Best_Selling_Week\":best_week, \"Qty\":[qty]})\n","  \n","\n","df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n","df = df.repartition(200) #to increase parallelism\n","\n","#Use the pandas function in group by\n","result = df.groupby([\"Brand\",\"Store\"]).applyInPandas(best_selling_week, schema=\"Store string, Brand string, Best_Selling_Week string, Qty float\")\n","display(result.head(10))"]},{"cell_type":"markdown","metadata":{},"source":["### Optional reading\n","The existing solution accelerator is made to run in AML with PRS, familiarize yourself the following scripts:\n","- [timeseries_utilites.py](https://github.com/microsoft/solution-accelerator-many-models/blob/master/Custom_Script/scripts/timeseries_utilities.py)\n","- [train.py](https://github.com/microsoft/solution-accelerator-many-models/blob/master/Custom_Script/scripts/train.py)\n","- [forecast.py](https://github.com/microsoft/solution-accelerator-many-models/blob/master/Custom_Script/scripts/forecast.py)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Training content"]},{"cell_type":"markdown","metadata":{},"source":["### The Map function"]},{"cell_type":"markdown","metadata":{},"source":["Map operations are performed with pandas instances by DataFrame.mapInPandas() in order to transform an iterator of pandas.DataFrame to another iterator of pandas.DataFrame that represents the current PySpark DataFrame and returns the result as a PySpark DataFrame.\n","\n","The underlying function takes and outputs an iterator of pandas.DataFrame. It can return the output of arbitrary length in contrast to some pandas UDFs such as Series to Series pandas UDF."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["spark.conf.set(' spark.sql.execution.arrow.maxRecordsPerBatch', 100)\n","# Default is 10000 which in some cases may defeat the purpose of parallelism"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>Brand</th><th>Week</th><th>Quantity</th><th>Revenue</th></tr></thead><tbody><tr><td>1094</td><td>minute.maid</td><td>24-1990</td><td>17892.0</td><td>37394.28</td></tr><tr><td>1094</td><td>minute.maid</td><td>25-1990</td><td>14053.0</td><td>34429.850000000006</td></tr><tr><td>1094</td><td>minute.maid</td><td>26-1990</td><td>17341.0</td><td>42832.270000000004</td></tr><tr><td>1094</td><td>minute.maid</td><td>27-1990</td><td>17194.0</td><td>41609.479999999996</td></tr><tr><td>1094</td><td>minute.maid</td><td>28-1990</td><td>17945.0</td><td>42888.55</td></tr><tr><td>1094</td><td>minute.maid</td><td>29-1990</td><td>17371.0</td><td>39953.299999999996</td></tr><tr><td>1094</td><td>minute.maid</td><td>30-1990</td><td>9825.0</td><td>23187.0</td></tr><tr><td>1094</td><td>minute.maid</td><td>31-1990</td><td>10849.0</td><td>27990.420000000002</td></tr><tr><td>1094</td><td>minute.maid</td><td>32-1990</td><td>12084.0</td><td>24168.0</td></tr><tr><td>1094</td><td>minute.maid</td><td>33-1990</td><td>10484.0</td><td>24322.879999999997</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["def parallel_transform(df_iterator):\n","  for df in df_iterator:\n","    df['Week'] = df['WeekStarting'].map(lambda x: str(x.isocalendar()[1]) +\"-\"+ str(x.isocalendar()[0]))\n","    df.drop(\"WeekStarting\", inplace=True, axis=1)\n","    yield df\n","df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n","\n","result = df.mapInPandas(parallel_transform, schema=\"Store string, Brand string, Week string, Quantity float, Revenue string\")\n","\n","display(result.head(10))"]},{"cell_type":"markdown","metadata":{},"source":["### Many Model Training"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#prepare values to broadcast\n","tenant_id = '' \n","service_principal_id=''\n","service_principal_password=dbutils.secrets.get('[SCOPENAME]','KEYNAME')\n","subscription_id = ''\n","# Azure Machine Learning resource group NOT the managed resource group\n","resource_group = '' \n","\n","#Azure Machine Learning workspace name, NOT Azure Databricks workspace\n","workspace_name = ''  "]},{"cell_type":"markdown","metadata":{},"source":["### Test with a single store & brand combination (single time series)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["%run ./timeseries_utilities\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#Get data\n","import pandas as pd\n","train_data_df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data where Store = '1066' and Brand ='tropicana'\").toPandas()\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Quantity</th><th>Brand</th><th>Revenue</th><th>Store</th></tr></thead><tbody><tr><td>1990-06-14T00:00:00.000+0000</td><td>13198.0</td><td>tropicana</td><td>29695.5</td><td>1066</td></tr><tr><td>1990-06-21T00:00:00.000+0000</td><td>12188.0</td><td>tropicana</td><td>27179.24</td><td>1066</td></tr><tr><td>1990-06-28T00:00:00.000+0000</td><td>10453.0</td><td>tropicana</td><td>25505.32</td><td>1066</td></tr><tr><td>1990-07-05T00:00:00.000+0000</td><td>13390.0</td><td>tropicana</td><td>35349.6</td><td>1066</td></tr><tr><td>1990-07-12T00:00:00.000+0000</td><td>12798.0</td><td>tropicana</td><td>29691.359999999997</td><td>1066</td></tr><tr><td>1990-07-19T00:00:00.000+0000</td><td>18476.0</td><td>tropicana</td><td>49146.16</td><td>1066</td></tr><tr><td>1990-07-26T00:00:00.000+0000</td><td>16244.0</td><td>tropicana</td><td>35087.04</td><td>1066</td></tr><tr><td>1990-08-02T00:00:00.000+0000</td><td>16057.0</td><td>tropicana</td><td>35807.11</td><td>1066</td></tr><tr><td>1990-08-09T00:00:00.000+0000</td><td>16888.0</td><td>tropicana</td><td>35127.04</td><td>1066</td></tr><tr><td>1990-08-16T00:00:00.000+0000</td><td>14045.0</td><td>tropicana</td><td>30056.300000000003</td><td>1066</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["display(train_data_df.head(10))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Featurized data example:\n","/databricks/spark/python/pyspark/sql/pandas/conversion.py:315: UserWarning: createDataFrame attempted Arrow optimization because &#39;spark.sql.execution.arrow.pyspark.enabled&#39; is set to true; however, failed by the reason below:\n","  Unable to convert the field Week_Year. If this column is not necessary, you may consider dropping it or converting to primitive type before the conversion.\n","Direct cause: Unsupported type in conversion from Arrow: uint32\n","Attempting non-optimization as &#39;spark.sql.execution.arrow.pyspark.fallback.enabled&#39; is set to true.\n","  warnings.warn(msg)\n","</div>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Quantity</th><th>Week_Year</th><th>lag_1</th><th>lag_2</th><th>lag_3</th><th>lag_4</th></tr></thead><tbody><tr><td>13198.0</td><td>24</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>12188.0</td><td>25</td><td>13198.0</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><td>10453.0</td><td>26</td><td>12188.0</td><td>13198.0</td><td>NaN</td><td>NaN</td></tr><tr><td>13390.0</td><td>27</td><td>10453.0</td><td>12188.0</td><td>13198.0</td><td>NaN</td></tr><tr><td>12798.0</td><td>28</td><td>13390.0</td><td>10453.0</td><td>12188.0</td><td>13198.0</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Get data for one table to test the utility function\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","target_column= 'Quantity'\n","timestamp_column= 'WeekStarting'\n","drop_columns=['Revenue', 'Store', 'Brand']\n","model_type= 'lr'\n","model_name=train_data_df['Store'][0]+\"_\"+train_data_df['Brand'][0]\n","test_size=20\n","# 1.0 Read the data from CSV - parse timestamps as datetime type and put the time in the index\n","data = train_data_df \\\n","        .set_index('WeekStarting') \\\n","        .sort_index(ascending=True)\n","\n","# 2.0 Split the data into train and test sets\n","train = data[:-test_size]\n","test = data[-test_size:]\n","\n","# 3.0 Create and fit the forecasting pipeline\n","# The pipeline will drop unhelpful features, make a calendar feature, and make lag features\n","lagger = SimpleLagger(target_column, lag_orders=[1, 2, 3, 4])\n","transform_steps = [('column_dropper', ColumnDropper(drop_columns)),\n","                   ('calendar_featurizer', SimpleCalendarFeaturizer()), ('lagger', lagger)]\n","forecaster = SimpleForecaster(transform_steps, LinearRegression(), target_column, timestamp_column)\n","forecaster.fit(train)\n","print('Featurized data example:')\n","display(forecaster.transform(train).head())\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">&lt;command-2941322682361995&gt;:228: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n","  forecasts_insamp = pd.Series()\n","&lt;command-2941322682361995&gt;:234: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n","  forecasts = pd.Series()\n","Out[9]: &lt;__main__.SimpleForecaster at 0x7f84badf2f70&gt;</div>"]},"metadata":{},"output_type":"display_data"}],"source":["from azureml.core.authentication import ServicePrincipalAuthentication\n","from azureml.core import Workspace\n","from azureml.core import Model\n","\n","import cloudpickle \n","\n","sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n","                                         service_principal_id=service_principal_id,\n","                                         service_principal_password=service_principal_password)\n","# Instantiate Azure Machine Learning workspace\n","ws = Workspace.get(name=workspace_name,\n","                   subscription_id=subscription_id,\n","                   resource_group=resource_group,auth= sp_auth)\n","\n","\n","# 4.0 Get predictions on test set\n","forecasts = forecaster.forecast(test)\n","compare_data = test.assign(forecasts=forecasts).dropna()\n","\n","# 5.0 Calculate accuracy metrics for the fit\n","mse = mean_squared_error(compare_data[target_column], compare_data['forecasts'])\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(compare_data[target_column], compare_data['forecasts'])\n","actuals = compare_data[target_column].values\n","preds = compare_data['forecasts'].values\n","mape = np.mean(np.abs((actuals - preds) / actuals) * 100)\n","\n","# 7.0 Train model with full dataset\n","forecaster.fit(data)\n","\n","# 8.0 Save the forecasting pipeline\n","with open(model_name, mode='wb') as file:\n","   cloudpickle.dump(forecaster, file)\n","\n","model = Model.register(workspace=ws, model_name=model_name, model_path=model_name, tags={'mse':str(mse), 'mape': str(mape), 'rmse': str(rmse)})\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Scale it up with many model training with function Pandas API"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#Prepare the core training function\n","\n","from azureml.core.authentication import ServicePrincipalAuthentication\n","from azureml.core import Workspace\n","from azureml.core import Model\n","import cloudpickle\n","\n","#do not use joblib to dump because it will have issue with multi-level object\n","def many_model_train(train_data_df):\n","  sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n","                                         service_principal_id=service_principal_id,\n","                                         service_principal_password=service_principal_password)\n","  # Instantiate Azure Machine Learning workspace\n","  ws = Workspace.get(name=workspace_name,\n","                     subscription_id=subscription_id,\n","                     resource_group=resource_group,\n","                     auth= sp_auth)\n","\n","\n","  target_column= 'Quantity'\n","  timestamp_column= 'WeekStarting'\n","  drop_columns=['Revenue', 'Store', 'Brand']\n","  #Get the store and brand. They are unique from the group so just the first value is sufficient\n","  store = train_data_df['Store'][0]\n","  brand = train_data_df['Brand'][0]\n","\n","  model_name=store+\"_\"+brand\n","  test_size=20\n","  # 1.0 Format the input data from group by, put the time in the index\n","  data = train_data_df \\\n","          .set_index('WeekStarting') \\\n","          .sort_index(ascending=True)\n","\n","  # 2.0 Split the data into train and test sets\n","  train = data[:-test_size]\n","  test = data[-test_size:]\n","\n","  # 3.0 Create and fit the forecasting pipeline\n","  # The pipeline will drop unhelpful features, make a calendar feature, and make lag features\n","  lagger = SimpleLagger(target_column, lag_orders=[1, 2, 3, 4])\n","  transform_steps = [('column_dropper', ColumnDropper(drop_columns)),\n","                     ('calendar_featurizer', SimpleCalendarFeaturizer()), ('lagger', lagger)]\n","  forecaster = SimpleForecaster(transform_steps, LinearRegression(), target_column, timestamp_column)\n","  forecaster.fit(train)\n","\n","  # 4.0 Get predictions on test set\n","  forecasts = forecaster.forecast(test)\n","  compare_data = test.assign(forecasts=forecasts).dropna()\n","\n","  # 5.0 Calculate accuracy metrics for the fit\n","  mse = mean_squared_error(compare_data[target_column], compare_data['forecasts'])\n","  rmse = np.sqrt(mse)\n","  mae = mean_absolute_error(compare_data[target_column], compare_data['forecasts'])\n","  actuals = compare_data[target_column].values\n","  preds = compare_data['forecasts'].values\n","  mape = np.mean(np.abs((actuals - preds) / actuals) * 100)\n","\n","  # 7.0 Train model with full dataset\n","  forecaster.fit(data)\n","\n","  # 8.0 Save the pipeline and register model to AML\n","  with open(model_name, mode='wb') as file:\n","     cloudpickle.dump(forecaster, file)#   \n","  model = Model.register(workspace=ws, model_name=model_name, model_path=model_name, tags={'mse':str(mse), 'mape': str(mape), 'rmse': str(rmse)})\n","  \n","  return pd.DataFrame({'Store':store,'Brand':brand, 'mse':[mse], 'mape': [mape], 'rmse': [rmse], 'model_name':[model_name]})\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n","df = df.repartition(200) #to increase parallelism\n","result = df.groupby([\"Brand\",\"Store\"]).applyInPandas(many_model_train, schema=\"Store string, Brand string, mse float, mape float, rmse float, model_name string \")"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store</th><th>Brand</th><th>mse</th><th>mape</th><th>rmse</th><th>model_name</th></tr></thead><tbody><tr><td>1031</td><td>tropicana</td><td>1.0501595E7</td><td>20.475067138671875</td><td>3240.616455078125</td><td>1031_tropicana</td></tr><tr><td>1021</td><td>minute.maid</td><td>8323296.5</td><td>17.98264503479004</td><td>2885.012451171875</td><td>1021_minute.maid</td></tr><tr><td>1074</td><td>tropicana</td><td>8422692.0</td><td>18.610090255737305</td><td>2902.1875</td><td>1074_tropicana</td></tr><tr><td>1077</td><td>minute.maid</td><td>1.2016312E7</td><td>22.2121524810791</td><td>3466.455322265625</td><td>1077_minute.maid</td></tr><tr><td>1078</td><td>minute.maid</td><td>6714000.0</td><td>13.475154876708984</td><td>2591.138671875</td><td>1078_minute.maid</td></tr><tr><td>1019</td><td>minute.maid</td><td>1.0599569E7</td><td>23.259544372558594</td><td>3255.69775390625</td><td>1019_minute.maid</td></tr><tr><td>1090</td><td>tropicana</td><td>5647451.5</td><td>15.766434669494629</td><td>2376.436767578125</td><td>1090_tropicana</td></tr><tr><td>1099</td><td>tropicana</td><td>8836949.0</td><td>19.134098052978516</td><td>2972.70068359375</td><td>1099_tropicana</td></tr><tr><td>1014</td><td>minute.maid</td><td>7331310.0</td><td>14.901358604431152</td><td>2707.63916015625</td><td>1014_minute.maid</td></tr><tr><td>1020</td><td>minute.maid</td><td>1.0048782E7</td><td>20.763259887695312</td><td>3169.9814453125</td><td>1020_minute.maid</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["display(result.head(10))"]},{"cell_type":"markdown","metadata":{},"source":["### Many Model Inferencing\n","Let's prepare a function pandas UDF to produce forecast for mutliple store and brand given the test data"]},{"cell_type":"markdown","metadata":{},"source":["#### Quick test the forecast function in utils with just one time series"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">&lt;command-2941322682361995&gt;:228: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n","  forecasts_insamp = pd.Series()\n","&lt;command-2941322682361995&gt;:234: DeprecationWarning: The default dtype for empty Series will be &#39;object&#39; instead of &#39;float64&#39; in a future version. Specify a dtype explicitly to silence this warning.\n","  forecasts = pd.Series()\n","</div>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Prediction</th><th>Store</th><th>Brand</th></tr></thead><tbody><tr><td>1990-06-14T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-06-21T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-06-28T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-05T00:00:00.000+0000</td><td>null</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-12T00:00:00.000+0000</td><td>14410.436058105348</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-19T00:00:00.000+0000</td><td>14569.927951313914</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-07-26T00:00:00.000+0000</td><td>14580.11668534655</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-08-02T00:00:00.000+0000</td><td>14873.030648117357</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-08-09T00:00:00.000+0000</td><td>15168.468388925961</td><td>1066</td><td>tropicana</td></tr><tr><td>1990-08-16T00:00:00.000+0000</td><td>14837.636937709074</td><td>1066</td><td>tropicana</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#Test forecast for one time series, need to run command 27-30 first\n","timeseries_id_columns= [ 'Store', 'Brand']\n","ts_id_dict = {id_col: str(data[id_col].iloc[0]) for id_col in timeseries_id_columns}\n","forecasts=forecaster.forecast(data)\n","prediction_df = forecasts.to_frame(name='Prediction')\n","prediction_df =prediction_df.reset_index().assign(**ts_id_dict)\n","display(prediction_df.head(10))"]},{"cell_type":"markdown","metadata":{},"source":["#### Main solution using map in pandas & loading models from AML workspace"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#Prepare the core forecast function in pandas function API  \n","from azureml.core.authentication import ServicePrincipalAuthentication\n","from azureml.core import Workspace\n","from azureml.core import Model\n","import cloudpickle\n","#do not use joblib to dump because it will have issue with multi-level object\n","\n","def many_model_forecast(input_data_df):\n","  sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n","                                         service_principal_id=service_principal_id,\n","                                         service_principal_password=service_principal_password)\n","  # Instantiate Azure Machine Learning workspace\n","  ws = Workspace.get(name=workspace_name,\n","                     subscription_id=subscription_id,\n","                     resource_group=resource_group,\n","                     auth= sp_auth)\n","\n","  timestamp_column= 'WeekStarting'\n","  timeseries_id_columns= [ 'Store', 'Brand']\n","  data = input_data_df \\\n","        .set_index(timestamp_column) \\\n","        .sort_index(ascending=True)\n","\n","  # Prepare loading model from Azure ML, get the latest model by default\n","  model_name=data['Store'][0]+\"_\"+data['Brand'][0]\n","  model = Model(ws, model_name)\n","  model.download(exist_ok =True)\n","  with open(model_name, 'rb') as f:\n","    forecaster = cloudpickle.load(f)\n","\n","  # Get predictions \n","  # This is to append the store and brand column to the result\n","  ts_id_dict = {id_col: str(data[id_col].iloc[0]) for id_col in timeseries_id_columns}\n","  forecasts=forecaster.forecast(data)\n","  prediction_df = forecasts.to_frame(name='Prediction')\n","  prediction_df =prediction_df.reset_index().assign(**ts_id_dict)\n","  \n","  return prediction_df\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["#Load data to score, for now, it's same train data but in reality, it should be different.\n","df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n","df = df.repartition(200) #to increase parallelism\n","prediction_result = df.groupby([\"Brand\",\"Store\"]).applyInPandas(many_model_forecast, schema=\"WeekStarting date, Store string, Brand string, Prediction float\")"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WeekStarting</th><th>Store</th><th>Brand</th><th>Prediction</th></tr></thead><tbody><tr><td>1990-06-14</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-06-21</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-06-28</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-07-05</td><td>1031</td><td>tropicana</td><td>null</td></tr><tr><td>1990-07-12</td><td>1031</td><td>tropicana</td><td>13858.3623046875</td></tr><tr><td>1990-07-19</td><td>1031</td><td>tropicana</td><td>13675.5673828125</td></tr><tr><td>1990-07-26</td><td>1031</td><td>tropicana</td><td>14244.9736328125</td></tr><tr><td>1990-08-02</td><td>1031</td><td>tropicana</td><td>14784.28515625</td></tr><tr><td>1990-08-09</td><td>1031</td><td>tropicana</td><td>14830.6640625</td></tr><tr><td>1990-08-16</td><td>1031</td><td>tropicana</td><td>14164.19140625</td></tr></tbody></table></div>"]},"metadata":{},"output_type":"display_data"}],"source":["display(prediction_result.head(10))"]}],"metadata":{"language_info":{"name":"python"},"name":"many_models_spark","notebookId":3319412928880917},"nbformat":4,"nbformat_minor":0}
